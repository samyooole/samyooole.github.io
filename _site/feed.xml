<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-01-19T22:20:26-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Samuel Ho</title><subtitle>Building bespoke tech solutions for the public good üå±</subtitle><entry><title type="html">A simple virtual reality 3D design tool made with Unity/C#</title><link href="http://localhost:4000/blog/jekyll/2025/01/10/vr.html" rel="alternate" type="text/html" title="A simple virtual reality 3D design tool made with Unity/C#" /><published>2025-01-10T20:00:00-05:00</published><updated>2025-01-10T20:00:00-05:00</updated><id>http://localhost:4000/blog/jekyll/2025/01/10/vr</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2025/01/10/vr.html"><![CDATA[<p>I recently completed <a href="https://test-classes.ssit.cucloud.net/browse/roster/FA24/class/INFO/5340">INFO 5340</a>, ‚ÄúVirtual and Augmented Reality‚Äù, at Cornell Tech. It was a pretty intense course which was very much focused on constant building week after week - in particular, in the 3D/game development context, with a particular focus on virtual reality. For our final group project, we were tasked with emulating features from a virtual headset 3D design tool, <a href="https://gravitysketch.com">Gravity Sketch</a>. While previous assignments had broken down our tasks into bite-sized pieces, the final project was assigned as-is: watch a video showing how a particular function is supposed to work, and replicate it any way you wish, making sure to respect fidelity to the original product + efficiency. This made the final project especially challenging, as we had to switch hats between product managing (how many features can we implement? what should we do first? who should take which function? is this feature of good quality? are there imperfections or lag? how would a real customer feel using this product?) as well as software engineering (how to efficiently/correctly implement a function? how to structure / refactor code properly? how to handle merge conflicts between different features worked on by different swes? how to catch and squash bugs?).</p>

<p>We were almost completely on our own, and I think this was what made it an especially rewarding experience, since we ultimately developed a pretty good intuition for vector graphics + state management and got pretty good practice in writing C# day in, day out.</p>

<h1 id="features">Features</h1>

<p>Without further ado, here were some of the features we managed to replicate! The goal with this project was to get the 3D design tool up to the point where it could create a simple car out of primitive shapes.</p>

<h2 id="multi-select">Multi-select</h2>

<p>The idea behind this feature is that there is a translucent sphere beside the right controller that would be responsible for grabbing objects. Whenever it intersected with an object, it would shine bright red to indicate that there were relevant grabbable objects. Furthermore, it would also be capable of grabbing multiple objects at a time (all objects that fell within its intersection).</p>

<p><img src="/assets/vr-multiselect.gif" alt="" /></p>

<p>At the same time, you can rescale the grab sphere in order to select as many or as few objects as you‚Äôd like.</p>

<p><img src="/assets/vr-spherewhoosh.gif" alt="" /></p>

<h2 id="create-object">Create object</h2>

<p>By pressing the right grip button, you can create an object. The controllers will track the distance between your left and right controllers, and create an object of the corresponding size.</p>

<p><img src="/assets/vr-simpleselect.gif" alt="" /></p>

<p>By pressing both grip buttons, you create an object non-uniformly. That means, the relevant height, width, and depth between your left and right controllers are kept track of in detail, resulting in the ability to create compressed/extended non-standard objects like cuboids or ovoids.</p>

<p><img src="/assets/vr-nonuniformselect.gif" alt="" /></p>

<p>You can also select the shape which you want to create.</p>

<p><img src="/assets/vr-selectshape.gif" alt="" /></p>

<h2 id="select-colour">Select colour</h2>

<p>When selecting an object (or multiple objects), clicking the left secondary button brings up a colour wheel from which users can select from a continuous range of colour. Releasing the button sets the shape to that particular colour.</p>

<p><img src="/assets/vr-color.gif" alt="" /></p>

<h2 id="grab-move">Grab move</h2>

<p>‚ÄúGrab move‚Äù is a special function in Gravity Sketch where you can press both trigger buttons simultaneously in order to change the scale, orientation, and position of all created objects. This provides a convenient alternative for designers to ‚Äúmove around‚Äù their designs. A helper percentage visual is also provided to provide context as to how much the user has scaled the design up or down.</p>

<p><img src="/assets/vr-grabmove.gif" alt="" /></p>

<h2 id="change-object-dimension">Change object dimension</h2>

<p>In this function, you can deform an object along a particular primary axis. Works for different primitive shapes!</p>

<p><img src="/assets/vr-axispull.gif" alt="" /></p>

<h1 id="some-lessons-learnt">Some lessons learnt</h1>

<h2 id="1-state-management-and-control-flow">1. State management and control flow</h2>

<p>Being super clear about your if/else/while statements is really important, as is overall state management. I learnt not to jump straight into coding a particular function, but to observe its functionality carefully and draw up an abstract control flow diagram. Then, write the code modularly, and abstractly state what you would expect your new code to do. If there are divergences, check for bugs, and iterate accordingly.</p>

<h2 id="2-input-moderation">2. Input moderation</h2>

<p>There are multiple functions which clash in terms of input buttons. For example, one function may require the right trigger button, while another may require both trigger buttons simultaneously. How do we make sure both inputs don‚Äôt clash? We had to make sure that when one particular state was active, that all other states could not be enabled. Furthermore, it was also a matter of user experience fine-tuning to see what made intuitive sense for when we would enter one function or the other, assuming neither were activated yet.</p>

<h2 id="3-having-a-balanced-holistic-approach-to-structuring-code-is-important">3. Having a balanced, holistic approach to structuring code is important</h2>

<p>It‚Äôs incredibly easy to end up writing everything within the same <code class="language-plaintext highlighter-rouge">Update()</code> function. It‚Äôs also equally easy to split every functionality up into their separate <code class="language-plaintext highlighter-rouge">Update()</code> functions, to the point where you‚Äôve lost track of what every other module is doing. Therefore, having a clear, abstracted view of what every module is doing and what elements they are manipulating under what conditions is pretty important, since many unexpected bugs appear from multiple modules manipulating the same element.</p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="CSharp" /><category term="Unity" /><summary type="html"><![CDATA[I recently completed INFO 5340, ‚ÄúVirtual and Augmented Reality‚Äù, at Cornell Tech. It was a pretty intense course which was very much focused on constant building week after week - in particular, in the 3D/game development context, with a particular focus on virtual reality. For our final group project, we were tasked with emulating features from a virtual headset 3D design tool, Gravity Sketch. While previous assignments had broken down our tasks into bite-sized pieces, the final project was assigned as-is: watch a video showing how a particular function is supposed to work, and replicate it any way you wish, making sure to respect fidelity to the original product + efficiency. This made the final project especially challenging, as we had to switch hats between product managing (how many features can we implement? what should we do first? who should take which function? is this feature of good quality? are there imperfections or lag? how would a real customer feel using this product?) as well as software engineering (how to efficiently/correctly implement a function? how to structure / refactor code properly? how to handle merge conflicts between different features worked on by different swes? how to catch and squash bugs?).]]></summary></entry><entry><title type="html">Simulating robots in digital twins - a basic ROS2/IsaacSim workflow</title><link href="http://localhost:4000/blog/jekyll/2025/01/10/isaacsim.html" rel="alternate" type="text/html" title="Simulating robots in digital twins - a basic ROS2/IsaacSim workflow" /><published>2025-01-10T20:00:00-05:00</published><updated>2025-01-10T20:00:00-05:00</updated><id>http://localhost:4000/blog/jekyll/2025/01/10/isaacsim</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2025/01/10/isaacsim.html"><![CDATA[<p>In this walkthrough, I will describe how I set up my Ubuntu system to simulate a simple search-for-human-and-move robot running on a ROS2 node within <a href="https://www.youtube.com/watch?v=lcee9ntkOjk">NVIDIA‚Äôs Isaac Sim</a>. The goal is to go through the entire pipeline, from importing a 3D model of the robot we are interested in, to articulating the joints/motors we wish to manipulate with our ROS node, to finally instantiating a ROS package and writing out the relevant code (in both Python and C++) within a node, and running it in conjunction with Isaac Sim to see if it works!</p>

<h3 id="motivation">Motivation</h3>

<p>First up, <em>what</em> is simulating robots in digital twins? In simulators like Isaac Sim, <a href="https://gazebosim.org/home">Gazebo</a>, <a href="https://mujoco.org/">MuJoCo</a> and more, we can test robots in high-fidelity environments driven by sophisticated graphics and physics engines. <em>Why</em> we want to do that is - assuming the simulation is <a href="https://www.andrew.cmu.edu/course/10-703/slides/Lecture_sim2realmaxentRL.pdf">sufficiently faithful</a> to reality - primarily because the debug cycle with real robots can be exponentially longer than simple on-computer development, thereby extending development time excessively. Furthermore, testing in real life can lead to wear and tear, as well as unexpected safety issues. From a reinforcement learning perspective, testing in simulation, when sped up and massively parallelized, can also lead to faster training across a larger number of samples.</p>

<h3 id="workflow-framework">Workflow framework</h3>

<p><img src="/assets/isaacsim-workflow.png" alt="" /></p>

<p>Let‚Äôs describe a classic write-and-test workflow (in orange) in real life. Usually, we‚Äôre given a template robot with pre-defined routines to control its joints/motors, or we physically build a robot and expose its joints/motors as endpoints to be manipulated by on-board processes. Then, we write some control loop (eg. a manually-defined finite state machine, optimal control or reinforcement learning algorithm, or some combination of all of them) within a ROS2 package composed of various nodes with separated concerns. Finally, we spin up the relevant nodes on the real robot, which publish to topics which manipulate control components, while subscribing to topics which receive data from sensors (eg. RGB camera, lidar).</p>

<p>We want to replace real-life articulation and testing in a simulator like Isaac Sim. For our simulated workflow (in red), we must (1) virtually articulate our robots‚Äô joints and motors, which is primarily done with a visual scripting tool called Action Graphs, then (2) write our ROS2 code as before to actually program control of the robot, then (3) test run in Isaac Sim. This may require extra steps like designing/importing a digital twin environment or assets like humans.</p>

<p>The overarching goal is to articulate our virtual robot as faithfully to a template robot we are provided, or else to build our physical robot as faithfully to our ideal virtual robot, so that we can write a ROS2 package that effortlessly runs similarly whether it‚Äôs in the simulation or in real life. This way, once we‚Äôve gotten past the fixed overhead associated with setting up the Isaac Sim environment/assets, we can quickly iterate on the crucial part - the ROS2 / robot control code - in sim, with the eventual goal of minimizing test time in reality.</p>

<h3 id="in-this-walkthrough">In this walkthrough‚Ä¶</h3>

<p>Now that we understand why we‚Äôre doing this, let me describe what I will be going through in this walkthrough.</p>

<p><strong>(1)</strong> <a href="#compatibly-installing-isaac-sim-and-ros2">A brief guide to (compatibly) installing Isaac Sim and ROS2</a></p>

<p><strong>(2)</strong> <a href="#environment-and-asset-setup">Importing a .urdf (robot model) to Isaac Sim + tuning + setting up the environment</a></p>

<p><strong>(3)</strong> <a href="#articulating-robots-motors">Articulating the virtual robot‚Äôs basic motors in Isaac Sim</a></p>

<p><strong>(4)</strong> <a href="#articulating-robots-camera">Articulating sensors, eg. cameras</a></p>

<p><strong>(5)</strong> <a href="#setting-up-a-ros2-package">Setting up a ROS2 package</a></p>

<p><strong>(6)</strong> <a href="#writing-ros2-nodes-for-a-simple-detect-and-follow-fsm">Writing ROS2 nodes for a simple detect-and-follow FSM</a></p>

<p><strong>(7)</strong> <a href="#results-in-isaac-sim">Results in Isaac Sim</a></p>

<h3 id="compatibly-installing-isaac-sim-and-ros2">Compatibly installing Isaac Sim and ROS2</h3>

<p>All details for installing Isaac Sim can be found <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/installation/index.html">here</a>. I won‚Äôt say much about this as it‚Äôs quite straightforward. I WILL say, however, that in order for it to be straightforward, the following is EXTREMELY recommended:</p>

<ul>
  <li><strong>Use <a href="https://releases.ubuntu.com/jammy/">Ubuntu 22.04</a></strong>. I tried finagling with Windows and WSL but there‚Äôs way too much overhead associated with getting different components to recognize each other. It‚Äôs <em>just faster</em> to create a partition for Ubuntu. Do note that you <em>have</em> to use Ubuntu 22.04 specifically because Isaac Sim only supports up to ROS2 Humble, which is not compatible with Ubuntu &gt;22.</li>
  <li><strong>Use ROS2 Humble</strong>. As stated before, ROS2 Humble is maximally compatible with Isaac Sim.</li>
  <li><strong>Remember to source ROS before opening Isaac Sim</strong>, and take note of install instructions to make sure ROS2 Bridge is working.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> /opt/ros/humble/setup.bash
</code></pre></div></div>

<h3 id="environment-and-asset-setup">Environment and asset setup</h3>

<p>The relevant official tutorials for this portion can be found <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/ros2_tutorials/tutorial_ros2_turtlebot.html">here</a>.</p>

<h4 id="setting-up-a-basic-environment">Setting up a basic environment</h4>

<p>Our robot needs a floor to stand on. If you‚Äôve completed the full <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/installation/install_workstation.html">workstation installation</a> for Isaac Sim, they provide plenty of environment/human/robot asset templates.</p>

<p>I shall import a basic floor.</p>

<p><img src="/assets/isaacsim-defaultenv.png" alt="" /></p>

<p>As you can see, in the window in the bottom, you can find lots of default templates in the Omniverse. Please follow the path in the above screenshot (may change from year to year, of course) and drag the <code class="language-plaintext highlighter-rouge">default_environment.usd</code> file into the blank space above.</p>

<h4 id="importing-a-custom-robot">Importing a custom robot</h4>

<p>I‚Äôm looking to Sim2Real a <a href="https://turtlebot.github.io/turtlebot4-user-manual/">turtlebot4</a>. Therefore, I‚Äôll have to find a .urdf model online myself.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/turtlebot/turtlebot4.git
</code></pre></div></div>

<p>Now, you‚Äôll find the .urdf file (a 3D model with some added physical modeling for control components like joints/motors) for the turtlebot4 in <code class="language-plaintext highlighter-rouge">turtlebot4/turtlebot4_description/urdf/standard/turtlebot4.urdf.xacro</code>.</p>

<p>Isaac Sim only reads .urdf files, so we‚Äôll have to convert this file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update and install converter along with dependencies</span>
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt <span class="nb">install </span>ros-humble-xacro
<span class="nb">sudo </span>apt <span class="nb">install </span>ros-humble-irobot-create-control

<span class="c"># Follow this format</span>
ros2 run xacro xacro <span class="nt">-o</span> &lt;filename&gt;.urdf &lt;filename&gt;.urdf.xacro
</code></pre></div></div>

<p>Now, you can drag this .urdf model into your Isaac Sim environment!</p>

<h4 id="tuning">Tuning</h4>

<p>You may have to tune some joints, motors or weights to make sure the robot is balanced/can move appropriately. For this turtlebot4 model, for some bizarre reason, there is a phantom mass in front of the turtlebot4 which helps it balance on the forward/backward axis. I have to increase its mass so that the bot is not leaning backwards.</p>

<p>Selecting the turtlebot4 in the sidebar, go to <code class="language-plaintext highlighter-rouge">turtlebot4&gt;front_caster_link</code>, and scroll down the bottom right property window, and set <code class="language-plaintext highlighter-rouge">Mass</code> to 1.5.</p>

<p><img src="/assets/isaacsim-mass.png" alt="" /></p>

<h3 id="articulating-robots-motors">Articulating robot‚Äôs motors</h3>

<p>Now that we‚Äôve imported our robot, we will need to articulate its motors. This basically means that we tell the computer that it can control those particular motors, and in our particular case, through ROS2 topics. The main way to do this in Isaac Sim is through a visual scripting tool called <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/ros2_tutorials/tutorial_ros2_drive_turtlebot.html">Action Graphs</a>, although options are also available to script these using Python. We‚Äôll focus on Action Graphs in this walkthrough since it‚Äôs pretty straightforward.</p>

<p>Create an Action Graph in <code class="language-plaintext highlighter-rouge">Window &gt; Visual Scripting &gt; Action Graph</code>. Then, add components to look something like this:</p>

<p><img src="/assets/isaacsim-moveactiongraph.png" alt="" /></p>

<p>We need to go to the <code class="language-plaintext highlighter-rouge">Articulation Controller</code>, pull up its property window, and set the <code class="language-plaintext highlighter-rouge">targetPrim</code> to be <code class="language-plaintext highlighter-rouge">/World/turtlebot4/base_link</code>. For your own custom imported robot, you will need to set a <code class="language-plaintext highlighter-rouge">targetPrim</code> to whatever has an <code class="language-plaintext highlighter-rouge">Articulation Root</code>.</p>

<p>Next, we need to go to <code class="language-plaintext highlighter-rouge">Constant Token</code>, and for one, set the <code class="language-plaintext highlighter-rouge">Value</code> to <code class="language-plaintext highlighter-rouge">left_wheel_joint</code>, and the other‚Äôs <code class="language-plaintext highlighter-rouge">Value</code> to <code class="language-plaintext highlighter-rouge">right_wheel_joint</code>. Name may vary depending on your custom imported robot, so just explore a bit.</p>

<p>The idea behind the above adjustments is that the <code class="language-plaintext highlighter-rouge">Articulation Controller</code> receives a velocity command from a ROS topic, and converts it to an actual physical action with respect to the relevant robot joints.</p>

<p>Next, in the <code class="language-plaintext highlighter-rouge">Differential Controller</code>, we follow the tutorial and set:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">maxLinearSpeed: 0.22</code></li>
  <li><code class="language-plaintext highlighter-rouge">wheelDistance: 0.16</code></li>
  <li><code class="language-plaintext highlighter-rouge">wheelRadius: 0.025</code></li>
</ul>

<p>To test everything is working fine, click the play button in Isaac Sim, and run in terminal</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 topic pub /cmd_vel geometry_msgs/Twist <span class="s2">"{'linear': {'x': 0.2, 'y': 0.0, 'z': 0.0}, 'angular': {'x': 0.0, 'y': 0.0, 'z': 0.0}}"</span>
</code></pre></div></div>

<p>The robot should be moving forward now.</p>

<h3 id="articulating-robots-camera">Articulating robot‚Äôs camera</h3>

<p>Now that we‚Äôve articulated the robot‚Äôs motors, we will need to create a camera which will capture RGB images and publish that to the <code class="language-plaintext highlighter-rouge">/rgb</code> topic. Down the line, we will be able to use this image for the purposes of human detection. In turtlebot4‚Äôs case, we want to put a camera on the little bracket at the back of the bot.</p>

<p>First, create a camera in Isaac Sim by selecing <code class="language-plaintext highlighter-rouge">Create &gt; Camera</code>. Then, parent it under <code class="language-plaintext highlighter-rouge">oakd_rgb_camera_frame</code> directly. If you‚Äôre using a custom robot, I faced quite a bit of problems trying to figure out the correct parent, but just keep going to the camera viewport and checking whether the image looks correct. Also, the camera begins by facing downwards, so to make it face forward you‚Äôll need to change <code class="language-plaintext highlighter-rouge">Orient &gt; X = 90</code> and <code class="language-plaintext highlighter-rouge">Orient &gt; Y = -90</code>.</p>

<p><img src="/assets/isaacsim_camerasettings.png" alt="" /></p>

<p>Once we‚Äôve gotten the camera in place, we will next articulate the camera with the goal of getting it to publish image data to the <code class="language-plaintext highlighter-rouge">/rgb</code> topic. Create an action graph that looks like the following:</p>

<p><img src="/assets/isaacsim-cameraactiongraph.png" alt="" /></p>

<p>In <code class="language-plaintext highlighter-rouge">Isaac Create Render Product</code>, set the target of the <code class="language-plaintext highlighter-rouge">cameraPrim</code> to the camera we just created and parented under the bracket above. This connects the sensors to the graph. Then, in <code class="language-plaintext highlighter-rouge">ROS2 Camera Helper</code>, set the <code class="language-plaintext highlighter-rouge">topicName</code> to ‚Äúrgb‚Äù, and <code class="language-plaintext highlighter-rouge">type</code> to ‚Äúrgb‚Äù as well. You can look at the options in the <code class="language-plaintext highlighter-rouge">type</code> and find many other options, like depth or bbox.</p>

<p>To test everything is working fine, click the play button in Isaac Sim, and run in terminal</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 topic list <span class="c"># check if /rgb is being published</span>
ros2 topic <span class="nb">echo</span> /rgb <span class="c"># this should print out many values - they are rgb values for each pixel</span>
</code></pre></div></div>

<p>Now, our simulated robot is controllable and can observe through its sensors! This is just a basic example, but there are many more components to explore: sensors, like lidar, or controllers, like joints. If you‚Äôd like a guide to articulating joints, you can watch this <a href="https://www.youtube.com/watch?v=dAOOo4uy_UY">helpful video</a>.</p>

<h3 id="setting-up-a-ros2-package">Setting up a ROS2 package</h3>

<p>Let‚Äôs move on to actually writing a program that observes and controls.</p>

<p>Create your workspace (these folder peculiarities are just ROS conventions you have to get used to):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir </span>hello_robot
<span class="nb">cd </span>hello_robot
<span class="nb">mkdir </span>src
</code></pre></div></div>

<p>In this walkthrough, we will be writing <em>primarily</em> in C++. So, here is the code to instantiate a ROS2 package:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 pkg create <span class="nt">--build-type</span> ament_cmake <span class="nt">--node-name</span> hello_robot_node hello_robot_package
</code></pre></div></div>

<p>What does a package do? It can contain multiple nodes, and is used to build all your programs consistently in one fell swoop.</p>

<p>Now, whenever you create a package, the first thing you‚Äôll want to think about are dependencies of your C++/Python code. In Python, you know off hand that you‚Äôll need to <code class="language-plaintext highlighter-rouge">pip install &lt;package&gt;</code>. In ROS2, this is analogous to doing a <code class="language-plaintext highlighter-rouge">sudo apt install &lt;package&gt;</code> <em>and then</em> updating 2 important files: <code class="language-plaintext highlighter-rouge">package.xml</code> and <code class="language-plaintext highlighter-rouge">CMakeLists.txt</code> (C++) / <code class="language-plaintext highlighter-rouge">setup.py</code> (Python).</p>

<p>Obviously you won‚Äôt know for sure what packages or dependencies you‚Äôll be using ahead of time, but the basic flow goes like this. Let‚Äôs say we want to use <a href="http://wiki.ros.org/geometry_msgs"><code class="language-plaintext highlighter-rouge">geometry_msgs</code></a>.</p>

<p>First, install the package at hand.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>ros-humble-geometry-msgs
</code></pre></div></div>

<p>Update <code class="language-plaintext highlighter-rouge">package.xml</code>, which can be found in <code class="language-plaintext highlighter-rouge">{workspace_folder}/src/{node_folder}/</code>:</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;depend&gt;</span>geometry_msgs<span class="nt">&lt;/depend&gt;</span>
</code></pre></div></div>

<p>Update <code class="language-plaintext highlighter-rouge">CMakeLists.txt</code>, if using C++:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp"># fill this in in the top section, along with the rest of the find_package
</span><span class="n">find_package</span><span class="p">(</span><span class="n">geometry_msgs</span> <span class="n">REQUIRED</span><span class="p">)</span>

<span class="cp"># fill this in after add_executable(...)
</span><span class="n">ament_target_dependencies</span><span class="p">(</span><span class="o">&lt;</span><span class="n">robot_node_name</span><span class="o">&gt;</span> <span class="n">rclcpp</span> <span class="n">geometry_msgs</span><span class="p">)</span>
</code></pre></div></div>

<p>In my particular case, I am using both Python and C++, so <a href="https://github.com/samyooole/hello-robot/blob/main/src/hello_robot_package/package.xml">here</a> is my <code class="language-plaintext highlighter-rouge">package.xml</code>, and <a href="https://github.com/samyooole/hello-robot/blob/main/src/hello_robot_package/CMakeLists.txt">here</a> is my <code class="language-plaintext highlighter-rouge">CMakeLists.txt</code> for your reference.</p>

<h3 id="writing-ros2-nodes-for-a-simple-detect-and-follow-fsm">Writing ROS2 nodes for a simple detect-and-follow FSM</h3>

<p>Now that we‚Äôve got the legwork out of the way, we want to write nodes which will fulfill a simple purpose: try to find a human, then move towards that human.</p>

<p>A simple FSM looks like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SEARCH -- if detect human --&gt; FORWARD
SEARCH -- if don't detect human --&gt; SEARCH
FORWARD -- if don't detect human --&gt; SEARCH
FORWARD -- if detect human --&gt; FORWARD
</code></pre></div></div>

<p>In my case, I choose to write the overall FSM in C++, which is in the main node file <code class="language-plaintext highlighter-rouge">{workspace_folder}/src/{node_folder}/src/hello_robot_node.cpp</code>. You can refer to my code <a href="https://github.com/samyooole/hello-robot/blob/main/src/hello_robot_package/src/hello_robot_node.cpp">here</a>.</p>

<p>I separate concerns by writing the human detection algorithm in Python, which is in a separate folder: <code class="language-plaintext highlighter-rouge">{workspace_folder}/src/{node_folder}/scripts/human_det_node.py</code>. You can refer to my code <a href="https://github.com/samyooole/hello-robot/blob/main/src/hello_robot_package/scripts/human_det_node.py">here</a>.</p>

<h4 id="important-basic-building-workflow-in-ros2">(IMPORTANT!) Basic building workflow in ROS2</h4>
<p>Once you‚Äôve copied all that code in, you can now build and run. Here is the basic workflow:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># in your overall workspace folder, run:</span>
<span class="nb">source</span> /opt/ros/humble/setup.bash <span class="c"># sources ROS</span>
colcon build <span class="c"># builds all changes in your node code</span>
<span class="nb">source install</span>/setup.bash <span class="c"># informs ROS of new dependencies etc</span>
ros2 run hello_robot_package human_det_node.py &amp; <span class="c"># run human detection node while also</span>
ros2 run hello_robot_package hello_robot_node <span class="c"># running the hello robot node</span>
</code></pre></div></div>

<h3 id="results-in-isaac-sim">Results in Isaac Sim</h3>

<p>Now, you can click play in Isaac Sim, and you should see the following results!</p>

<p><img src="/assets/isaacsim.gif" alt="" /></p>

<p>The idea then is that with a successful simulation, we can test it (and future iterations) on a real turtlebot4, and it should lead to similar results as well.</p>

<h3 id="credits-and-stuff">Credits and stuff</h3>

<p>Credits to this <a href="https://www.youtube.com/watch?v=dAOOo4uy_UY">Youtube video</a> for being a huge guide!</p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="ROS2" /><category term="C/C++" /><category term="Python" /><summary type="html"><![CDATA[In this walkthrough, I will describe how I set up my Ubuntu system to simulate a simple search-for-human-and-move robot running on a ROS2 node within NVIDIA‚Äôs Isaac Sim. The goal is to go through the entire pipeline, from importing a 3D model of the robot we are interested in, to articulating the joints/motors we wish to manipulate with our ROS node, to finally instantiating a ROS package and writing out the relevant code (in both Python and C++) within a node, and running it in conjunction with Isaac Sim to see if it works!]]></summary></entry><entry><title type="html">Apart, an extremely detailed political map of North Carolina</title><link href="http://localhost:4000/blog/jekyll/2024/08/02/apart.html" rel="alternate" type="text/html" title="Apart, an extremely detailed political map of North Carolina" /><published>2024-08-02T21:00:00-04:00</published><updated>2024-08-02T21:00:00-04:00</updated><id>http://localhost:4000/blog/jekyll/2024/08/02/apart</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2024/08/02/apart.html"><![CDATA[<p>When doing my senior thesis in Economics, I discovered a treasure trove of data. The North Carolina State Board of Elections maintains a <a href="https://www.ncsbe.gov/results-data/voter-registration-data">record</a> of public use, individual-level voter registration data for the past 15 years. Each individual is uniquely identified, characterized demographically, and located via home address. After geo-coding these home addresses, I was able to get the rooftop-level location of every single registered voter in North Carolina.</p>

<p>I then decided to develop a web app which would allow you to visualize where residents of different parties were staying, at different levels of detail. Enter‚Ä¶ <strong>apart</strong>!</p>

<p><img src="/assets/apart_closeup.png" alt="" />
<em>A residential neighborhood in Statesville, North Carolina</em></p>

<p>See the residential location of every single registered voter in North Carolina in 2020 - red for Republican, blue for Democrat, grey for unaffiliated.</p>

<p><img src="/assets/apart_afar.png" alt="" /></p>

<p>At larger zoom levels, see aggregations of Democrats vs Republicans on a broad scale. The redder the area, the more densely populated the area is by Republicans as compared to Democrats. The same is true for the Democrats if the area is bluer. Purple areas are therefore areas with a more even balance between Democrat and Republican residents.</p>

<h2 id="architecture">Architecture</h2>
<p><img src="/assets/apart_codearchi.png" alt="" /></p>

<p>I use ReactJS for the front-end, with react-leaflet as the map framework. Huge kudos to the team maintaining react-leaflet and the OpenStreetMap contributors behind the map tiles, it is truly a grand undertaking and it is amazing that it is open source and available for free. On broad zoom levels (&lt;16), summary map tiles (pre-rendered using a Python script) are loaded. On the smaller zoom levels, the bounds of the map view are used to send an API request to a simple NodeJS backend server for the relevant individual locations of party affiliates. The server then queries a MySQL database instance, delivering the records back to the ReactJS frontend, which then parses the fields and serves them as dots in various colors. Each of these will have to be deployed on dedicated servers, with the front-end on Vercel and the back-end (NodeJS + MySQL) on a VPS. The NodeJS server is run locally, and is then exposed via nginx to the public as an API.</p>

<p>Try apart <a href="https://apart-react.vercel.app">here</a>!</p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="Javascript" /><category term="SQL" /><category term="Python" /><summary type="html"><![CDATA[When doing my senior thesis in Economics, I discovered a treasure trove of data. The North Carolina State Board of Elections maintains a record of public use, individual-level voter registration data for the past 15 years. Each individual is uniquely identified, characterized demographically, and located via home address. After geo-coding these home addresses, I was able to get the rooftop-level location of every single registered voter in North Carolina.]]></summary></entry><entry><title type="html">ùÑûwatch, a chord detector that runs on Chrome</title><link href="http://localhost:4000/blog/jekyll/2024/07/17/swatch.html" rel="alternate" type="text/html" title="ùÑûwatch, a chord detector that runs on Chrome" /><published>2024-07-17T22:00:00-04:00</published><updated>2024-07-17T22:00:00-04:00</updated><id>http://localhost:4000/blog/jekyll/2024/07/17/swatch</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2024/07/17/swatch.html"><![CDATA[<p>As a musician, finding the chords for a song is something I do <em>extremely</em> often. Whether I‚Äôm arranging for acappella or simply replicating the backbone of a pop song on the piano, knowing the chords of a song are crucial for capturing its essential colour - and provides a contextual springboard from which more musical fun can happen: reharmonizations, new melody lines, mashups, and more!</p>

<p>Most times, you‚Äôll be able to find the chords for the song you want online on a website like <a href="https://www.ultimate-guitar.com">Ultimate Guitar</a>. But these chords tend to come with a set of issues - maybe they are simplified, or they weren‚Äôt transcribed accurately. Sometimes, there‚Äôs a song that doesn‚Äôt have any transcription at all because it‚Äôs simply not well-known enough!</p>

<p>In those moments, I wished there was a convenient tool to help me detect chords instantaneously. That is what spurred me to build <strong>ùÑûwatch</strong>, a Chrome extension that detects chords. It follows in the great tradition of lightweight Chrome extensions like <a href="https://www.colorzilla.com/chrome/">Colorzilla</a> and <a href="https://transpose.video">Transpose</a>, both effective semi-automated tech solutions which automate away the non-essential, thereby allowing the user to exercise their expert judgement. I wanted my extension to be able to instantaneously capture the key of a song, just as Colorzilla could capture the hex code of a colour. I also wanted my extension to perform digital signal processing natively within Chrome, much like Transpose could perform key changes on music on the fly.</p>

<p>Introducing‚Ä¶ <strong>ùÑûwatch!</strong></p>

<p><img src="/assets/swatch_atlast_demo.gif" alt="" />
<em><a href="https://www.youtube.com/watch?v=HUwhPN5-9bk">At Last</a>, originally by Etta James, performed by Cynthia Erivo</em></p>

<p>Play music from any audio source on a browser tab, and ùÑûwatch will near-instantaneously detect the musical chord for you, whilst also displaying the relevant notes that belong to that chord, so you can easily replicate it! For example, if C Major is detected, then C, E, and G will appear on the on-screen digital piano. Finally, you can also play and pause from within the Chrome extension, avoiding the need to exit the extension view every time you need to pause/play.</p>

<h2 id="architecture">Architecture</h2>

<p><img src="/assets/swatch_codearchi.png" alt="" /></p>

<p>This app was served to the user as a Chrome extension, which by construction uses HTML and CSS for the front-end design, while using Javascript for back-end scripting. Because one of my goals was to optimize the audio digital signal processing in a lower level language (in summary, I perform Fast Fourier Transforms, bin frequencies into <a href="https://en.wikipedia.org/wiki/Chroma_feature">chroma</a>, before minimizing similarity to established chord-chroma profiles) - I coded all the backend processing in C++. However, because C++ would traditionally be run in an executable file, I use <a href="https://webassembly.org">WebAssembly</a> to transpile C++ into browser-readable assembly code. The Javascript backend calls the .wasm code for compute-intensive tasks, while maintaining the broader averaging functionalities needed to assert more stability in chord detection.</p>

<p>The happy result of choosing this architecture is: firstly, there is minimal installation required. Once the extension has been downloaded and installed into one‚Äôs browser from the Chrome Web Store (and it is a tiny program), there is no further configuration required. Secondly, the extension functions standalone, that is to say it does not need to ping a server to get audio processing results. This is great because we don‚Äôt need to spend resources on maintaining a dedicated server, and we are also saving on latency costs, assuming that every user‚Äôs computer and Chrome browser is competent enough to execute the .wasm code produced. Thirdly, chord detection happens live and near-instantaneously. Because the underlying C++ is so speedy, there is no need to do any patch-up work on the backend with respect to time alignments, and musicians can get what they need immediately, which is crucial for their workflows. No one wants to be bogged down at the boring, grindy work - we want to give them as much time as possible for the real creative work.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There‚Äôs still a lot of work to be done on the DSP front when it comes to more precisely extracting chords. Clear-cut chords are easily identified correctly, but real songs with complicated chords are a much harder task that my extension does not yet excel at. Version 0.1 will soon be available on the Chrome Web Store for free.</p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="Javascript" /><category term="HTML/CSS" /><category term="C/C++" /><category term="WebAssembly" /><summary type="html"><![CDATA[As a musician, finding the chords for a song is something I do extremely often. Whether I‚Äôm arranging for acappella or simply replicating the backbone of a pop song on the piano, knowing the chords of a song are crucial for capturing its essential colour - and provides a contextual springboard from which more musical fun can happen: reharmonizations, new melody lines, mashups, and more!]]></summary></entry><entry><title type="html">Instabus, a customizable bus app for Singapore</title><link href="http://localhost:4000/blog/jekyll/2024/07/17/instabus.html" rel="alternate" type="text/html" title="Instabus, a customizable bus app for Singapore" /><published>2024-07-17T06:00:00-04:00</published><updated>2024-07-17T06:00:00-04:00</updated><id>http://localhost:4000/blog/jekyll/2024/07/17/instabus</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2024/07/17/instabus.html"><![CDATA[<p>Singapore has <em>excellent</em> public transport, but sometimes too much of a good thing can create a few relatively bad options. What do I mean?</p>

<p>While Singapore‚Äôs bus system is extensive, I am rather unfortunate to be living smack in the middle of a town block, something like this:</p>

<p><img src="/assets/idontlivehere.png" alt="" />
<em>I don‚Äôt actually live here, but let‚Äôs pretend I do</em></p>

<p>As you can see from this map, there are a healthy smattering of buses at different bus stops about equidistant from where I live, at the red x! The good thing about living here is I have plenty of options for the first/last mile of my journey. But give me 20 years of living in a place like this and one big problem starts to loom - which bus stop should I choose? Do I always go to the same bus stop and hopefully minimize my waiting time? Do I try to rifle through my bus app, with only one favourites tab with all the bus stops I care about, including all the buses I don‚Äôt care about?</p>

<p>This became enough of a frustration for me last summer that I set out to code an iOS app with a simple, central function: the <strong>movelist</strong>. Much like Spotify‚Äôs or Apple Music‚Äôs <em>play</em>list, you can gather the specific buses from different bus stops within the same grouping (eg. ‚Äúgoing to work‚Äù, ‚Äúcoming home from bus interchange‚Äù). This way, you are always only one tap away from seeing all the information you need to make a quick decision about which bus you should take in any given situation!</p>

<p>Enter‚Ä¶ Instabus!</p>

<div align="center">
    <img src="/assets/instabus_open_busstop.gif" width="300" />
</div>

<p>Much like any bus app out there, all the nearest bus stops are sorted by distance for easy access. Click the bus stop icon to reveal the available bus services and their next 3 bus arrival timings, color coded by the level of congestion within the bus (data is pulled from the <a href="https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf">LTA DataMall API</a>).</p>

<div align="center">
    <img src="/assets/instabus_createmovelist.gif" width="300" />
</div>

<p>To create a movelist, simply press the + button on any bus you‚Äôre interested in adding to the movelist - it will bring you to a landing page where you can choose to create a movelist and give it a special name! Conveniently, if you‚Äôve already taken a particular name, it will simply add that bus to that relevant movelist.</p>

<div align="center">
    <img src="/assets/instabus_addmovelist.gif" width="300" />
</div>

<p>Adding buses to a movelist is also simple once you have already created movelists!</p>

<div align="center">
    <img src="/assets/instabus_movelistscreen.gif" width="300" />
</div>

<p>Once you have all of your buses gathered in all the different movelists you need, looking at all of your buses in one screen is only one tap away! Now you can safely move towards the bus stop that you know will have a bus that comes fastest, as compared to all the other bus stops, without having to flip back and forth between different bus stops or having to sieve those buses across confusingly named bus stops!</p>

<h2 id="a-final-word">A final word</h2>
<p>This iOS app is coded entirely in Swift. There were many more features I explored but ultimately could not implement due to unchangeable limitations. For example, I wanted to implement a widget that would show me live bus arrival timings within each particular movelist. Sadly, I discovered from <a href="https://developer.apple.com/documentation/widgetkit/keeping-a-widget-up-to-date">Apple documentation</a> that the daily refresh budget for a widget is around 40 to 70 times a day. Even at the maximum budget, this would imply a minimum 20 minute refresh rate which was simply untenable for buses whose frequencies were on the order of 10 minutes, and whose bus arrival times can change dramatically across the span of 10 minutes owing to evolving traffic conditions.</p>

<p>I am now moving towards publishing Instabus on the App Store!</p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="Swift" /><summary type="html"><![CDATA[Singapore has excellent public transport, but sometimes too much of a good thing can create a few relatively bad options. What do I mean?]]></summary></entry><entry><title type="html">Behind SKIM: LLM-powered Contract AI</title><link href="http://localhost:4000/blog/jekyll/2024/01/13/skim.html" rel="alternate" type="text/html" title="Behind SKIM: LLM-powered Contract AI" /><published>2024-01-13T11:00:00-05:00</published><updated>2024-01-13T11:00:00-05:00</updated><id>http://localhost:4000/blog/jekyll/2024/01/13/skim</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2024/01/13/skim.html"><![CDATA[<p>In this app, I wanted to simplify a key step of the contract review process that many lawyers may have to go through - that is, sifting through the boilerplate in order to establish exactly what obligations are being established between both parties to a contract. You can try it out <a href="https://skim-react.vercel.app">here</a> - if you are using it for the first time, do wait a little bit for the API to warm up; I‚Äôm working on a college student‚Äôs budget right now ):</p>

<p><img src="/assets/skim-1.gif" alt="The app in a nutshell" />
<em>Put in any contract text in the text box, or as a word document‚Ä¶</em></p>

<p><img src="/assets/skim-2.gif" alt="filter" />
<em>Easily filter for the obligation category you are interested in</em></p>

<p>This seems like a task right up the alley of artificial intelligence - especially with the latest advancements in natural language processing in the form of large language models! In order to do that, I needed to first establish the key elements of the app that I was going to code up. It would have to take any length of not-necessarily-well-structured text, and be able to detect obligations, then classify them into a pre-defined set of categories that frequently appear across most contracts. These problems are not easy at all, and if you chunk them into their constitutent steps, each would comprise a significant amount of inference that would not have been possible until recently.</p>

<p>So, in this blog post, I will explain each major step of the app, showing what happens under the hood, and why I made the design/engineering choice that I did. Enjoy!</p>

<h2 id="text-classification-via-large-language-models-the-core-of-this-app">Text classification via large language models: the core of this app</h2>

<p>The core problem of this app was essentially multi-label classification - that is, if I looked at a sentence, I would need to bin it into one of these categories:</p>

<ol>
  <li>Termination condition</li>
  <li>Performance obligation</li>
  <li>Payment obligation</li>
  <li>Compliance obligation</li>
  <li>Support or warranty obligation</li>
  <li>Insurance obligation</li>
  <li>Quality obligation</li>
  <li>Audit obligation</li>
  <li>Contract renewal</li>
  <li>Exceptions or exemptions</li>
  <li>Indemnification</li>
  <li>Jurisdiction</li>
  <li>None of the above</li>
</ol>

<p>A huge problem with multi-label classification is that it‚Äôs difficult to be accurate - if we were to train this classifier in a more traditional way, we would not only need to have a lot of training data - we would need to have a lot of training data <em>for each label</em>. That can add up very quickly, and isn‚Äôt very easy for an independent programmer to do.</p>

<p>This is why large language models are so useful for this task - they are essentially an extended version of transfer learning, that is, using embeddings already trained on a massive corpus of the internet, and jumping off of <em>that</em> as the starting point for any further training. This is a process known as <strong>fine-tuning</strong>, and it has been really successful at allowing for training on few pieces of training data.</p>

<p>Before we train any further though, let‚Äôs see some of the capabilities of large language models - available for free, off the shelf at Hugging Face - at <em>zero-shot classification</em>. In order to do that, we convert large language model‚Äôs key functionality - text to text generation - and use prompt engineering in order to get specific, standardized answers that would be easy to parse within a backend, and thus coerce it into a text classification task. I tested this on Google‚Äôs <a href="https://huggingface.co/google/flan-t5-large">flan-t5-large</a>, and was using a GPU for inference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5ForConditionalGeneration</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">google/flan-t5-large</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">google/flan-t5-large</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># With GPU --&gt; .to('cuda')
</span>
<span class="sh">"</span><span class="s">It is expressly understood that Commerce may unilaterally cancel this Contract for Contractor‚Äôs refusal to comply with this provision.</span><span class="sh">"</span>

<span class="n">input_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
    Identify the clause type. If it is none, reply None

    Text: </span><span class="sh">"</span><span class="si">{</span><span class="n">clause_text</span><span class="si">}</span><span class="sh">"</span><span class="s">

    Contractual clause types:
    1. Termination condition
    2. Performance obligation
    3. Payment obligation
    4. Compliance obligation
    5. Support or warranty obligation
    6. Insurance obligation
    7. Quality obligation
    8. Audit obligation
    9. Contract renewal
    10. Exceptions or exemptions
    11. Indemnification
    12. Jurisdiction
    </span><span class="sh">"""</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
<span class="sh">"</span><span class="s">&lt;pad&gt; 1. &lt;/s&gt;</span><span class="sh">"</span>
</code></pre></div></div>

<p>Okay, that‚Äôs not too bad! It overall does a really great job at making predictions with zero prior information (provided by me) - but after looking at a full contract being processed there were two issues that stood out to me:</p>

<p>(1) there are a few clauses which are classified wrongly, and
(2) the answers are being returned in a format that I think is wrong (I want it to be returned as the text, not numbers)</p>

<p>It‚Äôs honestly really impressive that we‚Äôve gotten this far without really needing code (well - we needed code to download the model, but up till this point you could really just have used ChatGPT) - but the work awaits! Let‚Äôs fine-tune!</p>

<p>I get the zero-shot classifications and re-classify them by hand (for example, in the text I predicted earlier, I would write ‚ÄúTermination condition‚Äù as the desired output), and then I put that back in for fine-tuning:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">load_metric</span><span class="p">,</span> <span class="n">DatasetDict</span>

<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">T5Tokenizer</span>

<span class="n">zs</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="sh">'</span><span class="s">zeroshot.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># 90% train, 10% test + validation
</span><span class="n">train_testvalid</span> <span class="o">=</span> <span class="n">zs</span><span class="p">.</span><span class="nf">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Split the 10% test + valid in half test, half valid
</span><span class="n">test_valid</span> <span class="o">=</span> <span class="n">train_testvalid</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">].</span><span class="nf">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># gather everyone if you want to have a single DatasetDict
</span><span class="n">zs</span> <span class="o">=</span> <span class="nc">DatasetDict</span><span class="p">({</span>
    <span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">:</span> <span class="n">train_testvalid</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">:</span> <span class="n">test_valid</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">valid</span><span class="sh">'</span><span class="p">:</span> <span class="n">test_valid</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">]})</span>



<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="sh">'</span><span class="s">google/flan-t5-large</span><span class="sh">'</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
<span class="n">max_input_length</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># change to setting later
</span><span class="n">max_target_length</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># change to setting later
</span>

<span class="k">def</span> <span class="nf">preprocess_data</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>

  <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">clause_text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">clause</span><span class="sh">'</span><span class="p">]:</span>
    
    <span class="n">input_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
        Identify the clause type. If it is none, reply None.

        Text: </span><span class="sh">"</span><span class="si">{</span><span class="n">clause_text</span><span class="si">}</span><span class="sh">"</span><span class="s">

        Contractual clause types:
        1. Termination condition
        2. Performance obligation
        3. Payment obligation
        4. Compliance obligation
        5. Support or warranty obligation
        6. Insurance obligation
        7. Quality obligation
        8. Audit obligation
        9. Contract renewal
        10. Exceptions or exemptions
        11. Indemnification
        12. Jurisdiction
        </span><span class="sh">"""</span>
    
    <span class="n">inputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">)</span>

  <span class="n">model_inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Setup the tokenizer for targets
</span>  <span class="k">with</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">as_target_tokenizer</span><span class="p">():</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">flan_fewshot_cat</span><span class="sh">"</span><span class="p">],</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_target_length</span><span class="p">,</span> 
                       <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="n">model_inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">labels</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">model_inputs</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">zs</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">preprocess_data</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">DataCollatorForSeq2Seq</span><span class="p">,</span> <span class="n">Seq2SeqTrainingArguments</span><span class="p">,</span> <span class="n">Seq2SeqTrainer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">flan-t5-large-skim</span><span class="sh">"</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">bigdata/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="sh">"</span>

<span class="n">args</span> <span class="o">=</span> <span class="nc">Seq2SeqTrainingArguments</span><span class="p">(</span>
    <span class="n">model_dir</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">logging_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">4e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">predict_with_generate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="sh">"</span><span class="s">rouge1</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorForSeq2Seq</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>


<span class="c1"># Function that returns an untrained model to be trained
</span><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5ForConditionalGeneration</span>
<span class="k">def</span> <span class="nf">model_init</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">T5ForConditionalGeneration</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="nc">Seq2SeqTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="nf">model_init</span><span class="p">(),</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="sh">"</span><span class="s">valid</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
</code></pre></div></div>

<p>In order to save my work, I save it locally, and also push it to the Hugging Face Hub.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span><span class="p">.</span><span class="nf">save_model</span><span class="p">(</span><span class="sh">"</span><span class="s">bigdata/flan-t5-large-skim</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># save model locally
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">push_to_hub</span><span class="p">()</span> <span class="c1"># push to hugging face hub
</span></code></pre></div></div>

<p>The great thing about pushing it to the Hugging Face Hub is that they have a free inference API that allows you to host the computationally-heavy process on their servers instead of spinning up a server with enough RAM to fit large language models! The only issue is that there are rate limits, and they are not very forthcoming about the explicit limits‚Ä¶</p>

<h2 id="text-preprocessing">Text preprocessing</h2>

<p>Awesome! Now that we have our language model, surely we are done! Not so fast - a key step in designing apps is to understand the way in which your users will interact with your app. If I were a tired lawyer, I would not want to paste my obligations in one by one to check! That is why I designed my app to handle either long-form text being submitted through a text box, or even just a word document. Let‚Äôs take a look at some of the code that begins by parsing a word document:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Extract text from the .docx file
</span><span class="n">doc</span> <span class="o">=</span> <span class="nc">Document</span><span class="p">(</span><span class="n">docx_stream</span><span class="p">)</span>
<span class="n">clause_text</span> <span class="o">=</span> <span class="sh">""</span>

<span class="k">for</span> <span class="n">paragraph</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">paragraphs</span><span class="p">:</span>
    <span class="n">clause_text</span> <span class="o">+=</span> <span class="n">paragraph</span><span class="p">.</span><span class="n">text</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span>
</code></pre></div></div>

<p>Now, because text can be very unstructured (even if very readable!), I need to take a few text preprocessing steps in order to make sure my model isn‚Äôt inferring on garbage: empty strings, accidentally captured metadata, page numbers.</p>

<p>And before that, an even more important step: if you have noticed, my model presumes that I am sending in a single, complete sentence. That means that paragraphs will have to be broken up into sentences. This presents many problems, since rule-based methods, like splitting on periods, may end up accidentally catching periods within sentences (for example, decimal places/systems).</p>

<p>To solve this problem, I use <code class="language-plaintext highlighter-rouge">spacy</code>‚Äôs sentence boundary detection, which in turn uses dependency parsing to propagate from roots (eg. subject, object) down through their children (eg. adjectives, adverbs), till the end, which is then determined as a sentence boundary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span>

<span class="kn">from</span> <span class="n">spacy.cli</span> <span class="kn">import</span> <span class="n">download</span>

<span class="c1"># Specify the model name
</span><span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">en_core_web_sm</span><span class="sh">'</span>

<span class="c1"># Check if the model is already installed
</span><span class="k">if</span> <span class="n">model_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">spacy</span><span class="p">.</span><span class="n">util</span><span class="p">.</span><span class="nf">get_installed_models</span><span class="p">():</span>
    <span class="c1"># If not installed, download the model
</span>    <span class="nf">download</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">split_into_sentences</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">sents</span><span class="p">)</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">sentences</span>
</code></pre></div></div>

<p>Let‚Äôs see how this sentence boundary detection works with a tricky sentence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">Contractor shall allow public access to all documents, papers, letters or other materials made or received by Contractor in conjunction with this Contract, unless the records are exempt from section 24(a) of Article I of the State Constitution and section 119.07(1), F.S.  It is expressly understood that Commerce may unilaterally cancel this Contract for Contractor‚Äôs refusal to comply with this provision.</span><span class="sh">"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="n">text</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="nf">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">sents</span><span class="p">)]</span>

<span class="p">[</span><span class="sh">'</span><span class="s">Contractor shall allow public access to all documents, papers, letters or other materials made or received by Contractor in conjunction with this Contract, unless the records are exempt from section 24(a) of Article I of the State Constitution and section 119.07(1), F.S.  </span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">It is expressly understood that Commerce may unilaterally cancel this Contract for Contractor‚Äôs refusal to comply with this provision.</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<p>Great! Another major thing that I do is to strip out accidentally captured metadata, say if ‚ÄúPage 1 of 24‚Äù or ‚ÄúPage 2 of 24‚Äù or headers/footers accidentally make their way into the word document. In order to do this, I check if pairwise cosine similarities are above a particular threshold:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_distances</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>


<span class="k">def</span> <span class="nf">remove_repetitive_strings</span><span class="p">(</span><span class="n">strings</span><span class="p">,</span> <span class="n">similarity_threshold</span><span class="o">=</span><span class="mi">80</span><span class="p">):</span>
    <span class="c1"># Use TF-IDF Vectorizer to embed each line as a vector
</span>    <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">()</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">strings</span><span class="p">)</span>

    <span class="c1"># Calculate pairwise cosine distances
</span>    <span class="n">distances</span> <span class="o">=</span> <span class="nf">cosine_distances</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>

    <span class="c1"># Identify pairs with similarity below the threshold
</span>    <span class="n">filtered_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">strings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">strings</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">combinations</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">strings</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
                      <span class="k">if</span> <span class="n">distances</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">similarity_threshold</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)]</span>

    <span class="c1"># Create a set of strings to keep track of unique strings
</span>    <span class="n">unique_strings</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

    <span class="c1"># Add strings from filtered pairs to unique_strings
</span>    <span class="k">for</span> <span class="n">string1</span><span class="p">,</span> <span class="n">string2</span> <span class="ow">in</span> <span class="n">filtered_pairs</span><span class="p">:</span>
        <span class="n">unique_strings</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">string1</span><span class="p">)</span>
        <span class="n">unique_strings</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">string2</span><span class="p">)</span>

    <span class="c1"># Filter the original list based on unique strings
</span>    <span class="n">filtered_strings</span> <span class="o">=</span> <span class="p">[</span><span class="n">string</span> <span class="k">for</span> <span class="n">string</span> <span class="ow">in</span> <span class="n">strings</span> <span class="k">if</span> <span class="n">string</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unique_strings</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">filtered_strings</span>
</code></pre></div></div>

<p>There are some other basic things that I do, like stripping whitespaces or removing strings that don‚Äôt have any alphabetical content.</p>

<h2 id="front-end-to-back-end-an-overall-structure">Front-end to back-end: an overall structure</h2>

<p>The overall structure of the app is as follows: I run one instance of a ReactJS front-end on Vercel, and another instance of a Flask back-end on Azure. The Flask back-end handles all the text pre-processing and inference, whereas the ReactJS front-end passes text entered by the user on the web through a POST request towards the back-end, and receives the text.</p>

<p>There are many little details in the React app - like hovering over a clause text also highlights all other clause texts in the same category, and hovering over the category text at the top also highlights clause texts in that category, as well as other features like dynamically re-sizing text boxes. But there was one feature that was an interesting problem took a while to handle, and that was streaming.</p>

<p>You see, even the best large language models like ChatGPT take a while to complete inference. From the user‚Äôs standpoint, the total time of inference might be unacceptable, but they might be more willing to accept small chunks of the text being additively processed.</p>

<p>Therefore, in my use case, instead of waiting for the whole list of texts to be processed, I stream information back from the Flask app at every step of the loop using <code class="language-plaintext highlighter-rouge">yield</code>, and React is capable of processing this data while <code class="language-plaintext highlighter-rouge">await</code>ing the final set of data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Flask side:
</span><span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/api/generate_clause</span><span class="sh">'</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">POST</span><span class="sh">'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">generate_clause</span><span class="p">():</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
        <span class="n">progress</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">clause_text</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span>

        <span class="n">lotexts</span> <span class="o">=</span> <span class="nf">splitter</span><span class="p">(</span><span class="n">clause_text</span><span class="p">)</span> <span class="c1"># splits text according to preprocessing rules
</span>
        <span class="sh">"""</span><span class="s">
        Takes in a list of texts (lotexts) that has already been processed. could be a word doc file or just simple text, either way, they just need to be in an orderly list of texts.
        </span><span class="sh">"""</span>
        <span class="n">lodict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">text</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">lotexts</span><span class="p">):</span>
            <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">lotexts</span><span class="p">)</span>

            <span class="n">input_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
            Identify the clause type. If it is none, reply None

            Text: </span><span class="sh">"</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="s">

            Contractual clause types:
            1. Termination condition
            2. Performance obligation
            3. Payment obligation
            4. Compliance obligation
            5. Support or warranty obligation
            6. Insurance obligation
            7. Quality obligation
            8. Audit obligation
            9. Contract renewal
            10. Exceptions or exemptions
            11. Indemnification
            12. Jurisdiction
            </span><span class="sh">"""</span>

            <span class="c1"># error handling for model initialization
</span>            <span class="n">output</span> <span class="o">=</span> <span class="nf">query</span><span class="p">({</span>
                <span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_prompt</span><span class="p">,</span> <span class="sh">"</span><span class="s">wait_for_model</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span>
            <span class="p">})</span>

            <span class="n">category</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">generated_text</span><span class="sh">'</span><span class="p">]</span>

            <span class="n">lodict</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="n">text</span><span class="p">:</span> <span class="n">category</span><span class="p">})</span>

            <span class="n">json_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">text_block</span><span class="sh">'</span><span class="p">:</span> <span class="n">clause_text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lookup</span><span class="sh">'</span><span class="p">:</span> <span class="n">lodict</span><span class="p">,</span> <span class="sh">'</span><span class="s">progress</span><span class="sh">'</span><span class="p">:</span> <span class="n">progress</span><span class="p">}</span>
            <span class="n">json_string</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">json_data</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">json_string</span>  <span class="c1"># Add a special object to separate JSON streamed objects
</span>
    <span class="c1"># Use Response and stream_with_context to handle streaming
</span>    <span class="k">return</span> <span class="nc">Response</span><span class="p">(</span><span class="nf">stream_with_context</span><span class="p">(</span><span class="nf">generate</span><span class="p">()),</span>
                    <span class="n">content_type</span><span class="o">=</span><span class="sh">'</span><span class="s">text/event-stream</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="err">#</span> <span class="nx">React</span> <span class="nx">side</span>

<span class="kd">const</span> <span class="p">[</span><span class="nx">outputText</span><span class="p">,</span> <span class="nx">setOutputText</span><span class="p">]</span> <span class="o">=</span> <span class="nf">useState</span><span class="p">(</span><span class="dl">''</span><span class="p">);</span> <span class="c1">//text just gives the raw block text to be displayed</span>
<span class="kd">const</span> <span class="p">[</span><span class="nx">outputData</span><span class="p">,</span> <span class="nx">setOutputData</span><span class="p">]</span> <span class="o">=</span> <span class="nf">useState</span><span class="p">({})</span>
<span class="kd">const</span> <span class="p">[</span><span class="nx">progress</span><span class="p">,</span> <span class="nx">setProgress</span><span class="p">]</span> <span class="o">=</span> <span class="nf">useState</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="kd">const</span> <span class="nx">handleGenerateClick</span> <span class="o">=</span> <span class="k">async </span><span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="nx">url</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">http://localhost:5000/api/generate_clause</span><span class="dl">'</span><span class="p">;</span>
    <span class="kd">var</span> <span class="nx">tmpPromptResponse</span> <span class="o">=</span> <span class="dl">''</span><span class="p">;</span>
    <span class="k">try</span> <span class="p">{</span>
      <span class="kd">const</span> <span class="nx">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">fetch</span><span class="p">(</span><span class="nx">url</span> <span class="p">,</span> <span class="p">{</span>
        <span class="na">method</span><span class="p">:</span> <span class="dl">'</span><span class="s1">POST</span><span class="dl">'</span><span class="p">,</span>
        <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
          <span class="dl">'</span><span class="s1">Content-Type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">application/json</span><span class="dl">'</span>
        <span class="p">},</span>
        <span class="na">body</span><span class="p">:</span> <span class="nx">JSON</span><span class="p">.</span><span class="nf">stringify</span><span class="p">({</span>
          <span class="na">text</span><span class="p">:</span> <span class="nx">inputText</span><span class="p">,</span>
        <span class="p">}),</span>
      <span class="p">});</span>
      
      <span class="c1">// eslint-disable-next-line no-undef</span>
      <span class="kd">let</span> <span class="nx">decoder</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">TextDecoderStream</span><span class="p">();</span>
      <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">response</span><span class="p">.</span><span class="nx">body</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
      <span class="kd">const</span> <span class="nx">reader</span> <span class="o">=</span> <span class="nx">response</span><span class="p">.</span><span class="nx">body</span>
        <span class="p">.</span><span class="nf">pipeThrough</span><span class="p">(</span><span class="nx">decoder</span><span class="p">)</span>
        <span class="p">.</span><span class="nf">getReader</span><span class="p">();</span>
      
      <span class="k">while </span><span class="p">(</span><span class="kc">true</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="p">{</span><span class="nx">value</span><span class="p">,</span> <span class="nx">done</span><span class="p">}</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">reader</span><span class="p">.</span><span class="nf">read</span><span class="p">();</span>
        
        <span class="k">if </span><span class="p">(</span><span class="nx">done</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">break</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
          <span class="nx">tmpPromptResponse</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="nx">value</span><span class="p">);</span>
          <span class="nf">setOutputText</span><span class="p">(</span><span class="nx">tmpPromptResponse</span><span class="p">.</span><span class="nx">text_block</span><span class="p">);</span>
          <span class="nf">setOutputData</span><span class="p">(</span><span class="nx">tmpPromptResponse</span><span class="p">.</span><span class="nx">lookup</span><span class="p">);</span>
          <span class="nf">setProgress</span><span class="p">(</span><span class="nx">tmpPromptResponse</span><span class="p">.</span><span class="nx">progress</span><span class="p">);</span>

        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span> <span class="k">catch </span><span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">error</span><span class="p">);</span>
    <span class="p">}</span>

  <span class="p">};</span>

</code></pre></div></div>

<h2 id="a-final-word">A final word</h2>

<p>That brings me to the end of this blog! I think it was a pretty good experience handling everything from conceptualization, down to specifying a language model, testing and finetuning it, before setting it up on a Flask back-end server, and then designing a very preliminary user experience on a React front-end, and finally considering how all the moving pieces would fit together within a production context with compute power constraints (well, in my case, <em>severe</em> constraints).</p>

<p>A lot of what I could do on my GPU-enabled development run was rendered moot because of production constraints: for example, without using Hugging Face‚Äôs inference API, I could categorize documents a lot faster and without any limits. But we all must accept and work within the constraints provided, and this project was very instructive in the details of the full production process of an AI project.</p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="Python" /><summary type="html"><![CDATA[In this app, I wanted to simplify a key step of the contract review process that many lawyers may have to go through - that is, sifting through the boilerplate in order to establish exactly what obligations are being established between both parties to a contract. You can try it out here - if you are using it for the first time, do wait a little bit for the API to warm up; I‚Äôm working on a college student‚Äôs budget right now ):]]></summary></entry><entry><title type="html">Optimal control, trajectory optimization, and getting a rocket to space</title><link href="http://localhost:4000/blog/jekyll/2024/01/11/rockets.html" rel="alternate" type="text/html" title="Optimal control, trajectory optimization, and getting a rocket to space" /><published>2024-01-11T11:00:00-05:00</published><updated>2024-01-11T11:00:00-05:00</updated><id>http://localhost:4000/blog/jekyll/2024/01/11/rockets</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/2024/01/11/rockets.html"><![CDATA[<p>Let‚Äôs say we wanted to get a rocket to orbit - based on experience (or, if you‚Äôve made it through the Kerbal Space Program tutorial like me), you would roughly know that, with the rocket starting at an upward attitude with respect to the center of the Earth, you would want it to gradually tilt over to become increasingly parallel to the surface of the Earth. But what if we wanted to optimize some part of this flight process - let‚Äôs say, to make sure that we have as much fuel mass left as possible at the end of the journey? Well, the next thing we‚Äôd think about it, what are the factors of the flight that we can control, and how do these controls, on a physical basis, affect the state of the rocket (eg. the position of the rocket, the velocity of the rocket, the mass of the rocket)?</p>

<p>The simplest reducible example that we can think of is a car at rest at a starting position, which we want to get to a final position, also at rest. We specify an objective - for example, that we want it to reach its final position in the shortest possible time - and then ask what series of controls (applied forces) would optimize that objective. Intuitively, we would say - apply the maximum amount of force possible to get it to accelerate, before the applying the maximum amount of force possible in the opposite direction to get it to decelerate - and indeed, <a href="https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control">bang-bang solutions</a> are an optimal control.</p>

<p>There are a few special cases where analytical solutions can be found, but most don‚Äôt. This brings us to the field of numerical methods to approximate optimal control solutions - one of which is known as trajectory optimization!</p>

<p>In this app, I display the optimal control trajectory, given a starting position and final desired orbital parameters, using an algorithm known as trajectory optimization. You can try it <a href="https://rocket-optimal-control.streamlit.app/">here</a>.</p>

<p><img src="/assets/rocket.gif" alt="The app in a nutshell" /></p>

<h2 id="mathy-stuff-if-you-like-it">Mathy stuff if you like it</h2>

<p>In this section, I outline the optimal control problem specifically:</p>

<p><img src="/assets/optimalcontrol-1.png" alt="" />
<img src="/assets/optimalcontrol-2.png" alt="" />
<img src="/assets/optimalcontrol-3.png" alt="" /></p>]]></content><author><name></name></author><category term="blog" /><category term="jekyll" /><category term="Python" /><summary type="html"><![CDATA[Let‚Äôs say we wanted to get a rocket to orbit - based on experience (or, if you‚Äôve made it through the Kerbal Space Program tutorial like me), you would roughly know that, with the rocket starting at an upward attitude with respect to the center of the Earth, you would want it to gradually tilt over to become increasingly parallel to the surface of the Earth. But what if we wanted to optimize some part of this flight process - let‚Äôs say, to make sure that we have as much fuel mass left as possible at the end of the journey? Well, the next thing we‚Äôd think about it, what are the factors of the flight that we can control, and how do these controls, on a physical basis, affect the state of the rocket (eg. the position of the rocket, the velocity of the rocket, the mass of the rocket)?]]></summary></entry></feed>