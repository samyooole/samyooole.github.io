<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Behind SKIM: LLM-powered Contract AI</title>
  <link rel="stylesheet" href="/assets/main.css">


  <style>
    body {
        margin: 0; /* Remove default margin */
        padding: 0; /* Remove default padding */
      }
  
      .container {
        display: flex;
        border-top: 2px solid #ccc;
      }
  
  
      .sidebar {
        width: 20%;
        background-color: #f1f1f1;
        padding: 20px;
      }
  
      .project-box {
        
        text-align: center; /* Center the text within the box */
      }
  
      .project-image {
        max-width: 100%; /* Ensure the image doesn't exceed the box */
        height: auto; /* Maintain the aspect ratio of the image */
      }
  
      .content {
        flex: 1; /* Allow the content area to take the remaining width */
        padding: 20px;
      }
  
      header {
    text-align: center;
    padding: 20px;
    box-sizing: border-box;
    margin: 0 auto; /* Center the header horizontally */
  }
  
  .site-title {
    text-decoration: none;
    color: white;
    font-size: 3em;
  }
  
  
  </style>



</head>
<body>


    <header>
      <a class="site-title" rel="author" href="/">samuel ho 🌱</a>
    </header>
  
  <!-- Container for Sidebar and Content -->
  <div class="container">
  
    <!-- Sidebar for Projects -->
    <nav class="sidebar">
      <div class="wrapper">
        <h2></h2>
        
  
        <!-- Project Box -->
        <div class="project-box">
          <a href="https://skim-react.vercel.app/" target="_blank" rel="noopener noreferrer">
            <img class="project-image" src="/assets/skim_icon.png" alt="Skim">
            <p>Skim: LLM-powered contract AI</p>
          </a>

          <a href="https://pdd-vrp.streamlit.app/" target="_blank" rel="noopener noreferrer">
            <img class="project-image" src="/assets/vrp_icon.png" alt="PDD-VRP">
            <p>Delivery robots: vehicle routing problem</p>
          </a>

          <a href="https://rocket-optimal-control.streamlit.app/" target="_blank" rel="noopener noreferrer">
            <img class="project-image" src="/assets/rocket_icon.png" alt="rocket">
            <p>Getting a rocket to orbit: an optimal control problem</p>
          </a>
        </div>
  
  
      </div>
    </nav>
  
    <!-- Main Content Area for Blog Post -->
<main class="content">
    <div class="wrapper">
      <article>
        <h1>Behind SKIM: LLM-powered Contract AI</h1>
        <p class="meta">January 13, 2022</p>
        <p>In this app, I wanted to simplify a key step of the contract review process that many lawyers may have to go through - that is, sifting through the boilerplate in order to establish exactly what obligations are being established between both parties to a contract. You can try it out <a href="https://skim-react.vercel.app">here</a> - if you are using it for the first time, do wait a little bit for the API to warm up; I’m working on a college student’s budget right now ):</p>

<p><img src="/assets/skim-1.gif" alt="The app in a nutshell" />
<em>Put in any contract text in the text box, or as a word document…</em></p>

<p><img src="/assets/skim-2.gif" alt="filter" />
<em>Easily filter for the obligation category you are interested in</em></p>

<p>This seems like a task right up the alley of artificial intelligence - especially with the latest advancements in natural language processing in the form of large language models! In order to do that, I needed to first establish the key elements of the app that I was going to code up. It would have to take any length of not-necessarily-well-structured text, and be able to detect obligations, then classify them into a pre-defined set of categories that frequently appear across most contracts. These problems are not easy at all, and if you chunk them into their constitutent steps, each would comprise a significant amount of inference that would not have been possible until recently.</p>

<p>So, in this blog post, I will explain each major step of the app, showing what happens under the hood, and why I made the design/engineering choice that I did. Enjoy!</p>

<h2 id="text-classification-via-large-language-models-the-core-of-this-app">Text classification via large language models: the core of this app</h2>

<p>The core problem of this app was essentially multi-label classification - that is, if I looked at a sentence, I would need to bin it into one of these categories:</p>

<ol>
  <li>Termination condition</li>
  <li>Performance obligation</li>
  <li>Payment obligation</li>
  <li>Compliance obligation</li>
  <li>Support or warranty obligation</li>
  <li>Insurance obligation</li>
  <li>Quality obligation</li>
  <li>Audit obligation</li>
  <li>Contract renewal</li>
  <li>Exceptions or exemptions</li>
  <li>Indemnification</li>
  <li>Jurisdiction</li>
  <li>None of the above</li>
</ol>

<p>A huge problem with multi-label classification is that it’s difficult to be accurate - if we were to train this classifier in a more traditional way, we would not only need to have a lot of training data - we would need to have a lot of training data <em>for each label</em>. That can add up very quickly, and isn’t very easy for an independent programmer to do.</p>

<p>This is why large language models are so useful for this task - they are essentially an extended version of transfer learning, that is, using embeddings already trained on a massive corpus of the internet, and jumping off of <em>that</em> as the starting point for any further training. This is a process known as <strong>fine-tuning</strong>, and it has been really successful at allowing for training on few pieces of training data.</p>

<p>Before we train any further though, let’s see some of the capabilities of large language models - available for free, off the shelf at Hugging Face - at <em>zero-shot classification</em>. In order to do that, we convert large language model’s key functionality - text to text generation - and use prompt engineering in order to get specific, standardized answers that would be easy to parse within a backend, and thus coerce it into a text classification task. I tested this on Google’s <a href="https://huggingface.co/google/flan-t5-large">flan-t5-large</a>, and was using a GPU for inference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5ForConditionalGeneration</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"google/flan-t5-large"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"google/flan-t5-large"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span> <span class="c1"># With GPU --&gt; .to('cuda')
</span>
<span class="s">"It is expressly understood that Commerce may unilaterally cancel this Contract for Contractor’s refusal to comply with this provision."</span>

<span class="n">input_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
    Identify the clause type. If it is none, reply None

    Text: "</span><span class="si">{</span><span class="n">clause_text</span><span class="si">}</span><span class="s">"

    Contractual clause types:
    1. Termination condition
    2. Performance obligation
    3. Payment obligation
    4. Compliance obligation
    5. Support or warranty obligation
    6. Insurance obligation
    7. Quality obligation
    8. Audit obligation
    9. Contract renewal
    10. Exceptions or exemptions
    11. Indemnification
    12. Jurisdiction
    """</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
<span class="s">"&lt;pad&gt; 1. &lt;/s&gt;"</span>
</code></pre></div></div>

<p>Okay, that’s not too bad! It overall does a really great job at making predictions with zero prior information (provided by me) - but after looking at a full contract being processed there were two issues that stood out to me:</p>

<p>(1) there are a few clauses which are classified wrongly, and
(2) the answers are being returned in a format that I think is wrong (I want it to be returned as the text, not numbers)</p>

<p>It’s honestly really impressive that we’ve gotten this far without really needing code (well - we needed code to download the model, but up till this point you could really just have used ChatGPT) - but the work awaits! Let’s fine-tune!</p>

<p>I get the zero-shot classifications and re-classify them by hand (for example, in the text I predicted earlier, I would write “Termination condition” as the desired output), and then I put that back in for fine-tuning:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">load_metric</span><span class="p">,</span> <span class="n">DatasetDict</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">T5Tokenizer</span>

<span class="n">zs</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">'csv'</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="s">'zeroshot.csv'</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">"train"</span><span class="p">)</span>
<span class="c1"># 90% train, 10% test + validation
</span><span class="n">train_testvalid</span> <span class="o">=</span> <span class="n">zs</span><span class="p">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Split the 10% test + valid in half test, half valid
</span><span class="n">test_valid</span> <span class="o">=</span> <span class="n">train_testvalid</span><span class="p">[</span><span class="s">'test'</span><span class="p">].</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># gather everyone if you want to have a single DatasetDict
</span><span class="n">zs</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">({</span>
    <span class="s">'train'</span><span class="p">:</span> <span class="n">train_testvalid</span><span class="p">[</span><span class="s">'train'</span><span class="p">],</span>
    <span class="s">'test'</span><span class="p">:</span> <span class="n">test_valid</span><span class="p">[</span><span class="s">'test'</span><span class="p">],</span>
    <span class="s">'valid'</span><span class="p">:</span> <span class="n">test_valid</span><span class="p">[</span><span class="s">'train'</span><span class="p">]})</span>



<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s">'google/flan-t5-large'</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
<span class="n">max_input_length</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># change to setting later
</span><span class="n">max_target_length</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># change to setting later
</span>

<span class="k">def</span> <span class="nf">preprocess_data</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>

  <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">clause_text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s">'clause'</span><span class="p">]:</span>
    
    <span class="n">input_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Identify the clause type. If it is none, reply None.

        Text: "</span><span class="si">{</span><span class="n">clause_text</span><span class="si">}</span><span class="s">"

        Contractual clause types:
        1. Termination condition
        2. Performance obligation
        3. Payment obligation
        4. Compliance obligation
        5. Support or warranty obligation
        6. Insurance obligation
        7. Quality obligation
        8. Audit obligation
        9. Contract renewal
        10. Exceptions or exemptions
        11. Indemnification
        12. Jurisdiction
        """</span>
    
    <span class="n">inputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">)</span>

  <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Setup the tokenizer for targets
</span>  <span class="k">with</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">as_target_tokenizer</span><span class="p">():</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s">"flan_fewshot_cat"</span><span class="p">],</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_target_length</span><span class="p">,</span> 
                       <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="n">model_inputs</span><span class="p">[</span><span class="s">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">model_inputs</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">zs</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">preprocess_data</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">DataCollatorForSeq2Seq</span><span class="p">,</span> <span class="n">Seq2SeqTrainingArguments</span><span class="p">,</span> <span class="n">Seq2SeqTrainer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s">"flan-t5-large-skim"</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"bigdata/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">"</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">Seq2SeqTrainingArguments</span><span class="p">(</span>
    <span class="n">model_dir</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">logging_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">4e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">predict_with_generate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="s">"rouge1"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorForSeq2Seq</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>


<span class="c1"># Function that returns an untrained model to be trained
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5ForConditionalGeneration</span>
<span class="k">def</span> <span class="nf">model_init</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">T5ForConditionalGeneration</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Seq2SeqTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_init</span><span class="p">(),</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s">"train"</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s">"valid"</span><span class="p">],</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div></div>

<p>In order to save my work, I save it locally, and also push it to the Hugging Face Hub.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="s">"bigdata/flan-t5-large-skim"</span><span class="p">)</span> <span class="c1"># save model locally
</span><span class="n">trainer</span><span class="p">.</span><span class="n">push_to_hub</span><span class="p">()</span> <span class="c1"># push to hugging face hub
</span></code></pre></div></div>

<p>The great thing about pushing it to the Hugging Face Hub is that they have a free inference API that allows you to host the computationally-heavy process on their servers instead of spinning up a server with enough RAM to fit large language models! The only issue is that there are rate limits, and they are not very forthcoming about the explicit limits…</p>

<h2 id="text-preprocessing">Text preprocessing</h2>

<p>Awesome! Now that we have our language model, surely we are done! Not so fast - a key step in designing apps is to understand the way in which your users will interact with your app. If I were a tired lawyer, I would not want to paste my obligations in one by one to check! That is why I designed my app to handle either long-form text being submitted through a text box, or even just a word document. Let’s take a look at some of the code that begins by parsing a word document:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Extract text from the .docx file
</span><span class="n">doc</span> <span class="o">=</span> <span class="n">Document</span><span class="p">(</span><span class="n">docx_stream</span><span class="p">)</span>
<span class="n">clause_text</span> <span class="o">=</span> <span class="s">""</span>

<span class="k">for</span> <span class="n">paragraph</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">paragraphs</span><span class="p">:</span>
    <span class="n">clause_text</span> <span class="o">+=</span> <span class="n">paragraph</span><span class="p">.</span><span class="n">text</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span>
</code></pre></div></div>

<p>Now, because text can be very unstructured (even if very readable!), I need to take a few text preprocessing steps in order to make sure my model isn’t inferring on garbage: empty strings, accidentally captured metadata, page numbers.</p>

<p>And before that, an even more important step: if you have noticed, my model presumes that I am sending in a single, complete sentence. That means that paragraphs will have to be broken up into sentences. This presents many problems, since rule-based methods, like splitting on periods, may end up accidentally catching periods within sentences (for example, decimal places/systems).</p>

<p>To solve this problem, I use <code class="language-plaintext highlighter-rouge">spacy</code>’s sentence boundary detection, which in turn uses dependency parsing to propagate from roots (eg. subject, object) down through their children (eg. adjectives, adverbs), till the end, which is then determined as a sentence boundary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">spacy</span>

<span class="kn">from</span> <span class="nn">spacy.cli</span> <span class="kn">import</span> <span class="n">download</span>

<span class="c1"># Specify the model name
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">'en_core_web_sm'</span>

<span class="c1"># Check if the model is already installed
</span><span class="k">if</span> <span class="n">model_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">spacy</span><span class="p">.</span><span class="n">util</span><span class="p">.</span><span class="n">get_installed_models</span><span class="p">():</span>
    <span class="c1"># If not installed, download the model
</span>    <span class="n">download</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">split_into_sentences</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">sents</span><span class="p">)</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">sentences</span>
</code></pre></div></div>

<p>Let’s see how this sentence boundary detection works with a tricky sentence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"Contractor shall allow public access to all documents, papers, letters or other materials made or received by Contractor in conjunction with this Contract, unless the records are exempt from section 24(a) of Article I of the State Constitution and section 119.07(1), F.S.  It is expressly understood that Commerce may unilaterally cancel this Contract for Contractor’s refusal to comply with this provision."</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="n">text</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">sents</span><span class="p">)]</span>

<span class="p">[</span><span class="s">'Contractor shall allow public access to all documents, papers, letters or other materials made or received by Contractor in conjunction with this Contract, unless the records are exempt from section 24(a) of Article I of the State Constitution and section 119.07(1), F.S.  '</span><span class="p">,</span> <span class="s">'It is expressly understood that Commerce may unilaterally cancel this Contract for Contractor’s refusal to comply with this provision.'</span><span class="p">]</span>
</code></pre></div></div>

<p>Great! Another major thing that I do is to strip out accidentally captured metadata, say if “Page 1 of 24” or “Page 2 of 24” or headers/footers accidentally make their way into the word document. In order to do this, I check if pairwise cosine similarities are above a particular threshold:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_distances</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>


<span class="k">def</span> <span class="nf">remove_repetitive_strings</span><span class="p">(</span><span class="n">strings</span><span class="p">,</span> <span class="n">similarity_threshold</span><span class="o">=</span><span class="mi">80</span><span class="p">):</span>
    <span class="c1"># Use TF-IDF Vectorizer to embed each line as a vector
</span>    <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">strings</span><span class="p">)</span>

    <span class="c1"># Calculate pairwise cosine distances
</span>    <span class="n">distances</span> <span class="o">=</span> <span class="n">cosine_distances</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>

    <span class="c1"># Identify pairs with similarity below the threshold
</span>    <span class="n">filtered_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">strings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">strings</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">strings</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
                      <span class="k">if</span> <span class="n">distances</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">similarity_threshold</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)]</span>

    <span class="c1"># Create a set of strings to keep track of unique strings
</span>    <span class="n">unique_strings</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="c1"># Add strings from filtered pairs to unique_strings
</span>    <span class="k">for</span> <span class="n">string1</span><span class="p">,</span> <span class="n">string2</span> <span class="ow">in</span> <span class="n">filtered_pairs</span><span class="p">:</span>
        <span class="n">unique_strings</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">string1</span><span class="p">)</span>
        <span class="n">unique_strings</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">string2</span><span class="p">)</span>

    <span class="c1"># Filter the original list based on unique strings
</span>    <span class="n">filtered_strings</span> <span class="o">=</span> <span class="p">[</span><span class="n">string</span> <span class="k">for</span> <span class="n">string</span> <span class="ow">in</span> <span class="n">strings</span> <span class="k">if</span> <span class="n">string</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unique_strings</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">filtered_strings</span>
</code></pre></div></div>

<p>There are some other basic things that I do, like stripping whitespaces or removing strings that don’t have any alphabetical content.</p>

<h2 id="front-end-to-back-end-an-overall-structure">Front-end to back-end: an overall structure</h2>

<p>The overall structure of the app is as follows: I run one instance of a ReactJS front-end on Vercel, and another instance of a Flask back-end on Azure. The Flask back-end handles all the text pre-processing and inference, whereas the ReactJS front-end passes text entered by the user on the web through a POST request towards the back-end, and receives the text.</p>

<p>There are many little details in the React app - like hovering over a clause text also highlights all other clause texts in the same category, and hovering over the category text at the top also highlights clause texts in that category, as well as other features like dynamically re-sizing text boxes. But there was one feature that was an interesting problem took a while to handle, and that was streaming.</p>

<p>You see, even the best large language models like ChatGPT take a while to complete inference. From the user’s standpoint, the total time of inference might be unacceptable, but they might be more willing to accept small chunks of the text being additively processed.</p>

<p>Therefore, in my use case, instead of waiting for the whole list of texts to be processed, I stream information back from the Flask app at every step of the loop using <code class="language-plaintext highlighter-rouge">yield</code>, and React is capable of processing this data while <code class="language-plaintext highlighter-rouge">await</code>ing the final set of data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Flask side:
</span><span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">route</span><span class="p">(</span><span class="s">'/api/generate_clause'</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s">'POST'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">generate_clause</span><span class="p">():</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
        <span class="n">progress</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">clause_text</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">[</span><span class="s">'text'</span><span class="p">]</span>

        <span class="n">lotexts</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">(</span><span class="n">clause_text</span><span class="p">)</span> <span class="c1"># splits text according to preprocessing rules
</span>
        <span class="s">"""
        Takes in a list of texts (lotexts) that has already been processed. could be a word doc file or just simple text, either way, they just need to be in an orderly list of texts.
        """</span>
        <span class="n">lodict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">text</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lotexts</span><span class="p">):</span>
            <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">lotexts</span><span class="p">)</span>

            <span class="n">input_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
            Identify the clause type. If it is none, reply None

            Text: "</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s">"

            Contractual clause types:
            1. Termination condition
            2. Performance obligation
            3. Payment obligation
            4. Compliance obligation
            5. Support or warranty obligation
            6. Insurance obligation
            7. Quality obligation
            8. Audit obligation
            9. Contract renewal
            10. Exceptions or exemptions
            11. Indemnification
            12. Jurisdiction
            """</span>

            <span class="c1"># error handling for model initialization
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">query</span><span class="p">({</span>
                <span class="s">"inputs"</span><span class="p">:</span> <span class="n">input_prompt</span><span class="p">,</span> <span class="s">"wait_for_model"</span><span class="p">:</span> <span class="bp">True</span>
            <span class="p">})</span>

            <span class="n">category</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">'generated_text'</span><span class="p">]</span>

            <span class="n">lodict</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="n">text</span><span class="p">:</span> <span class="n">category</span><span class="p">})</span>

            <span class="n">json_data</span> <span class="o">=</span> <span class="p">{</span><span class="s">'text_block'</span><span class="p">:</span> <span class="n">clause_text</span><span class="p">,</span> <span class="s">'lookup'</span><span class="p">:</span> <span class="n">lodict</span><span class="p">,</span> <span class="s">'progress'</span><span class="p">:</span> <span class="n">progress</span><span class="p">}</span>
            <span class="n">json_string</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">json_data</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">json_string</span>  <span class="c1"># Add a special object to separate JSON streamed objects
</span>
    <span class="c1"># Use Response and stream_with_context to handle streaming
</span>    <span class="k">return</span> <span class="n">Response</span><span class="p">(</span><span class="n">stream_with_context</span><span class="p">(</span><span class="n">generate</span><span class="p">()),</span>
                    <span class="n">content_type</span><span class="o">=</span><span class="s">'text/event-stream'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="err">#</span> <span class="nx">React</span> <span class="nx">side</span>

<span class="kd">const</span> <span class="p">[</span><span class="nx">outputText</span><span class="p">,</span> <span class="nx">setOutputText</span><span class="p">]</span> <span class="o">=</span> <span class="nx">useState</span><span class="p">(</span><span class="dl">''</span><span class="p">);</span> <span class="c1">//text just gives the raw block text to be displayed</span>
<span class="kd">const</span> <span class="p">[</span><span class="nx">outputData</span><span class="p">,</span> <span class="nx">setOutputData</span><span class="p">]</span> <span class="o">=</span> <span class="nx">useState</span><span class="p">({})</span>
<span class="kd">const</span> <span class="p">[</span><span class="nx">progress</span><span class="p">,</span> <span class="nx">setProgress</span><span class="p">]</span> <span class="o">=</span> <span class="nx">useState</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="kd">const</span> <span class="nx">handleGenerateClick</span> <span class="o">=</span> <span class="k">async</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="nx">url</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">http://localhost:5000/api/generate_clause</span><span class="dl">'</span><span class="p">;</span>
    <span class="kd">var</span> <span class="nx">tmpPromptResponse</span> <span class="o">=</span> <span class="dl">''</span><span class="p">;</span>
    <span class="k">try</span> <span class="p">{</span>
      <span class="kd">const</span> <span class="nx">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">fetch</span><span class="p">(</span><span class="nx">url</span> <span class="p">,</span> <span class="p">{</span>
        <span class="na">method</span><span class="p">:</span> <span class="dl">'</span><span class="s1">POST</span><span class="dl">'</span><span class="p">,</span>
        <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
          <span class="dl">'</span><span class="s1">Content-Type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">application/json</span><span class="dl">'</span>
        <span class="p">},</span>
        <span class="na">body</span><span class="p">:</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">({</span>
          <span class="na">text</span><span class="p">:</span> <span class="nx">inputText</span><span class="p">,</span>
        <span class="p">}),</span>
      <span class="p">});</span>
      
      <span class="c1">// eslint-disable-next-line no-undef</span>
      <span class="kd">let</span> <span class="nx">decoder</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">TextDecoderStream</span><span class="p">();</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">response</span><span class="p">.</span><span class="nx">body</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
      <span class="kd">const</span> <span class="nx">reader</span> <span class="o">=</span> <span class="nx">response</span><span class="p">.</span><span class="nx">body</span>
        <span class="p">.</span><span class="nx">pipeThrough</span><span class="p">(</span><span class="nx">decoder</span><span class="p">)</span>
        <span class="p">.</span><span class="nx">getReader</span><span class="p">();</span>
      
      <span class="k">while</span> <span class="p">(</span><span class="kc">true</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="p">{</span><span class="nx">value</span><span class="p">,</span> <span class="nx">done</span><span class="p">}</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">reader</span><span class="p">.</span><span class="nx">read</span><span class="p">();</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="nx">done</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">break</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
          <span class="nx">tmpPromptResponse</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">value</span><span class="p">);</span>
          <span class="nx">setOutputText</span><span class="p">(</span><span class="nx">tmpPromptResponse</span><span class="p">.</span><span class="nx">text_block</span><span class="p">);</span>
          <span class="nx">setOutputData</span><span class="p">(</span><span class="nx">tmpPromptResponse</span><span class="p">.</span><span class="nx">lookup</span><span class="p">);</span>
          <span class="nx">setProgress</span><span class="p">(</span><span class="nx">tmpPromptResponse</span><span class="p">.</span><span class="nx">progress</span><span class="p">);</span>

        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">error</span><span class="p">);</span>
    <span class="p">}</span>

  <span class="p">};</span>

</code></pre></div></div>

<h2 id="a-final-word">A final word</h2>

<p>That brings me to the end of this blog! I think it was a pretty good experience handling everything from conceptualization, down to specifying a language model, testing and finetuning it, before setting it up on a Flask back-end server, and then designing a very preliminary user experience on a React front-end, and finally considering how all the moving pieces would fit together within a production context with compute power constraints (well, in my case, <em>severe</em> constraints).</p>

<p>A lot of what I could do on my GPU-enabled development run was rendered moot because of production constraints: for example, without using Hugging Face’s inference API, I could categorize documents a lot faster and without any limits. But we all must accept and work within the constraints provided, and this project was very instructive in the details of the full production process of an AI project.</p>

      </article>
    </div>
  </main>
  
        </div>
      </main><footer class="site-footer h-card">
  
  </footer>
</body>
</html>

<script src=”https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=”MathJax-script” async src=”https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>